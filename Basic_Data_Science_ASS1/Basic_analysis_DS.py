# -*- coding: utf-8 -*-
"""Assignment_part_one_LauraFabbiano.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Bx7DbdG0FZwRPvXWCRR02Q-ULz12-w7

#DSML investigation:
### You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.
### Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!
We have retrieved from the laptop of a spy agent some documents. Our intelligence shows that the person that we are looking for has visited countries X and Y, was in USA between Sept 2019 to Oct 2020. He is currently working undercover.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
userRecords = pd.read_csv("https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%201/data/userRecords.csv")
travelRecords = pd.read_csv("https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%201/data/flightRecords.csv")

"""#### Shape of the data

Let's first check how many rows and columns (features) are in the user records
"""

r, c = userRecords.shape
print("rows = ", r)
print("columns = ", c)

"""
#### Check out the first few rows
To protect innocent people, the name of the suspects have been censored, those will be revealed once number of potential suspects decreases.
"""

userRecords.head()

"""#### List the column/feature names"""

for col in userRecords.columns:
  print(col)

"""#### Duplicates
You can check if there are duplicates in the dataset.
"""

is_dup  = userRecords.duplicated().any()

"""Show the duplicated rows if any:"""

if is_dup :
  duplicates = userRecords[userRecords.duplicated()]
  print(duplicates)

"""Drop the duplicate entries if any"""

if is_dup :
    userRecords = userRecords.drop_duplicates() #we remove the duplicates in the original file : so we redefine the right database without the duplicates

#to make sure that the duplicates have been deleted, we can do :

is_dup = userRecords.duplicated().any()
print(is_dup)

"""## Spies often use false identities with fabricated data.
####This means that if some fields are appearing a statistically unusual amount of times, they may be fabricated
In this part we are going to verify if there are birthdates that have been reused an unusual amount of times
Lets first ensure that the birthday column has the correct format:
"""

#Check the data type of each feature
data_types = userRecords.dtypes
print(data_types)

"""Convert the culumn "birthday" to datetime if necessary"""

userRecords['birthday'] = pd.to_datetime(userRecords['birthday'], errors='coerce')

#now we can check if the data type has changed :
data_types = userRecords.dtypes
print(data_types['birthday'])

"""Find the list of birthdays that are duplicated/reused in the dataset"""

dup_birthday = userRecords[userRecords['birthday'].duplicated()]
print(dup_birthday['birthday'])

"""Let's count how many people have their birthday on the same day among the duplicated birthdays."""

birthday_counts = {}

for birthday in dup_birthday['birthday']:
    count = userRecords[userRecords['birthday'] == birthday].shape[0]
    birthday_counts[birthday] = count

for birthday, count in birthday_counts.items():
    print(birthday, count)

"""We can consider that there is no suspicious descrepancy through birthdays.
##We have reason to believe that the spy from whom we took the laptop suspected some people more than others and collected additionnal data about them.
By consequence we can exclude from the list of suspects the users with incomplete informations."""


null_counts = userRecords.isnull().sum()
print(null_counts)

"""
We will now proceed to identify the users who were in the USA between Sept 2019 to Oct 2020.

We will combine the flight record with the user record in order to ensure that users who have been unsuspected in the previous step are not considered
"""

#check the two dataset
userRecords.head()

travelRecords.head()

"""[Merging](https://pandas.pydata.org/docs/user_guide/merging.html) the two datasets:"""

#I need to convert UserID into userID otherwise i cannot merge the two databases based on the userID
userRecords = userRecords.rename(columns={'UserID': 'userID'})

merged_data = travelRecords.merge(userRecords, on='userID', how='outer')

"""We can now drop the travel values for users who have been removed from the suspect list prior."""

merged_data = merged_data.dropna()
merged_data.head()

"""Let's now identify the user who has travelled to the United States (US) between the 1st of september 2019 and the 31st of october 2020

*Note that the travel date is not in datetime format*
"""

merged_data['date'] = pd.to_datetime(merged_data['date'], format='%d-%m-%Y')

start_date = pd.to_datetime('01-09-2019', format='%d-%m-%Y')
end_date = pd.to_datetime('31-10-2020', format='%d-%m-%Y')

new_data = merged_data[(merged_data['date'] >= start_date) & (merged_data['date'] <= end_date) & (merged_data['destination'] == 'US')]
print(new_data['userID'])

"""## [Data visualisation ](https://pandas.pydata.org/docs/user_guide/visualization.html)

 Suisse Impossible Mission Force is happy with the result and would like to have some visual aid to make a profile of the potential suspects

Let's first make a pie chart to visualise the different employments held from the current suspects:
"""

new_data.groupby(['employment']).size().plot(kind='pie', autopct='%1.1f%%')
plt.title("Different employments from current suspects")

plt.show()

"""
We can use boxplots to identify outliers per employment category.

Let's first make two tables, one with the date of arrival and one with the date of departure (you can reuse your previous dataset for the date of arrival)
"""

arrival_dates = new_data.rename(columns={'date': 'date_arrival'})

departure_dates = merged_data[(merged_data['date'] >= start_date) & (merged_data['date'] <= end_date) & (merged_data['departure'] == 'US')]
departure_dates = departure_dates.rename(columns={'date': 'date_dep'})

"""We can keep on both these tables the user ID, employment, and departure date / arrival date."""

departure_dates = departure_dates[['userID', 'employment', 'date_dep']]
arrival_dates = arrival_dates[['userID', 'employment', 'date_arrival']]

"""We can now join the two tables on user ID"""

arr_dep_table = arrival_dates.merge(departure_dates, on='userID', how='inner')
arr_dep_table = arr_dep_table.drop(columns='employment_x')
arr_dep_table = arr_dep_table.rename(columns={'employment_y' : 'employment'})
arr_dep_table.head()

"""We can now create a new feature called journey_length representing the arrival date - departure date"""

arr_dep_table['journey_length'] = (arr_dep_table['date_dep'] - arr_dep_table['date_arrival']).dt.days
arr_dep_table.head()

"""We can now create the boxplots"""

arr_dep_table['journey_length'] = arr_dep_table['journey_length']
arr_dep_table.boxplot(column='journey_length', by='employment')

plt.ylabel('Days in USA')
plt.title('Journey length per employment')
plt.xticks(rotation=45)

#tried this technique that i found on google to find any outliers but without any success. so put the code in comment
#Q1 = arr_dep_table['journey_length'].quantile(0.25)
#Q3 = arr_dep_table['journey_length'].quantile(0.75)
#IQR = Q3 - Q1
#lower_bound = Q1 - 1.5 * IQR
#upper_bound = Q3 + 1.5 * IQR

#outliers = arr_dep_table[(arr_dep_table['journey_length'] < lower_bound) | (arr_dep_table['journey_length'] > upper_bound)]
#plt.scatter(outliers.index, outliers['journey_length'], color='red', label='Outliers')

plt.show()

"""##SIMF is satisfied with your work but believes the current data is inconclusive.

Your investigation will continue when more intelligence comes in...
"""

