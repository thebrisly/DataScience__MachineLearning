{"cells":[{"cell_type":"markdown","source":["# Text Analytics Tokenization - Stemming - Lemmatization\n","\n","There are two main packages for doing text processing and analytics with Python: NLTK and spaCy. SpaCy is the \"new kid on the block\" and should superior and faster results than NLTK, but for starting in text analytics both packages are good.\n","\n","We will play a bit with spaCy and NLTK."],"metadata":{"id":"elyFltX-oVkt"}},{"cell_type":"code","source":["# we update and install spaCy\n","!pip install -U spacy"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Collecting spacy\n","  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n","  Downloading weasel-0.3.3-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n","  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n","Installing collected packages: cloudpathlib, weasel, spacy\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.6.1\n","    Uninstalling spacy-3.6.1:\n","      Successfully uninstalled spacy-3.6.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.3\n"]}],"execution_count":1,"metadata":{"id":"CQr7v6fNhoHA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047725109,"user_tz":-120,"elapsed":11997,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}},"outputId":"3485ff4f-124a-4216-9a7d-a8714531df59"}},{"cell_type":"code","source":["# we load the english language model\n","!python -m spacy download en"],"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-23 07:55:32.423324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-10-23 07:55:34.269220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n","full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n","Collecting en-core-web-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.0) (3.7.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.3)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.66.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.23.5)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.3)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.6.0\n","    Uninstalling en-core-web-sm-3.6.0:\n","      Successfully uninstalled en-core-web-sm-3.6.0\n","Successfully installed en-core-web-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"execution_count":2,"metadata":{"id":"YZ_pEAARjZyF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047755141,"user_tz":-120,"elapsed":30041,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}},"outputId":"e50aa152-aaf3-41e7-e498-e93096841c32"}},{"cell_type":"markdown","source":["Load the spaCy language model."],"metadata":{"id":"KG8LhYtyipIh"}},{"cell_type":"code","source":["import spacy\n","sp = spacy.load('en_core_web_sm')"],"outputs":[],"execution_count":3,"metadata":{"id":"xNAhMeLliGX2","executionInfo":{"status":"ok","timestamp":1698047767148,"user_tz":-120,"elapsed":12047,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Tokenization\n","\n","We create a simple spaCy document.\n"],"metadata":{"id":"quYzhhajjJVu"}},{"cell_type":"code","source":["sentence = sp(u'The Royal Swedish Academy of Sciences has awarded the Nobel Prize in Physics 2019 \"for contributions to our understanding of the evolution of the universe and Earth''s place in the cosmos\"')"],"outputs":[],"execution_count":4,"metadata":{"id":"9P63E582ij0B","executionInfo":{"status":"ok","timestamp":1698047767494,"user_tz":-120,"elapsed":387,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["SpaCy automatically breaks your document into tokens when a document is created using the model.\n","\n","A token simply refers to an individual part of a sentence having some semantic value. Let's see what tokens we have in our document:"],"metadata":{"id":"0N9yjY7hjzUQ"}},{"cell_type":"code","source":["for word in sentence:\n","    print(word.text)"],"outputs":[{"output_type":"stream","name":"stdout","text":["The\n","Royal\n","Swedish\n","Academy\n","of\n","Sciences\n","has\n","awarded\n","the\n","Nobel\n","Prize\n","in\n","Physics\n","2019\n","\"\n","for\n","contributions\n","to\n","our\n","understanding\n","of\n","the\n","evolution\n","of\n","the\n","universe\n","and\n","Earths\n","place\n","in\n","the\n","cosmos\n","\"\n"]}],"execution_count":5,"metadata":{"id":"E39zh5bwjRGp","outputId":"b15f1131-50ff-47c8-9849-a7e691ee8ddc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047767494,"user_tz":-120,"elapsed":31,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["You can see we have the following tokens in our document. We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute shown below. POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context."],"metadata":{"id":"mGl8bsV6j7k8"}},{"cell_type":"code","source":["s1= sp(\"I like to fish\")\n","\n","for word in s1:\n","    print(word.text,  word.pos_)\n","print()\n","s2= sp(\"The fish jumped out of my hand\")\n","for word in s2:\n","    print(word.text,  word.pos_)\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["I PRON\n","like VERB\n","to PART\n","fish VERB\n","\n","The DET\n","fish NOUN\n","jumped VERB\n","out ADP\n","of ADP\n","my PRON\n","hand NOUN\n"]}],"execution_count":6,"metadata":{"id":"iXBkABx6j2RW","outputId":"4a227c98-d443-4448-a265-2372ed34fb54","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047767495,"user_tz":-120,"elapsed":28,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["You see how powerful POS tagging is!\n","\n","Let's visualize it.\n","\n"],"metadata":{"id":"ioXnaXJ9kLk-"}},{"cell_type":"code","source":["from spacy import displacy\n","\n","sen = sp(u\"The fish jumped out of my hand\")\n","displacy.render(sen, style='dep', jupyter=True, options={'distance': 85})"],"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"78709a30c196493fb87e650e325bf424-0\" class=\"displacy\" width=\"645\" height=\"222.0\" direction=\"ltr\" style=\"max-width: none; height: 222.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"135\">fish</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"135\">NOUN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"220\">jumped</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"220\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"305\">out</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"305\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"390\">of</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"390\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"475\">my</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"475\">PRON</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"560\">hand</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"560\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-78709a30c196493fb87e650e325bf424-0-0\" stroke-width=\"2px\" d=\"M70,87.0 C70,44.5 130.0,44.5 130.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-78709a30c196493fb87e650e325bf424-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,89.0 L62,77.0 78,77.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-78709a30c196493fb87e650e325bf424-0-1\" stroke-width=\"2px\" d=\"M155,87.0 C155,44.5 215.0,44.5 215.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-78709a30c196493fb87e650e325bf424-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M155,89.0 L147,77.0 163,77.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-78709a30c196493fb87e650e325bf424-0-2\" stroke-width=\"2px\" d=\"M240,87.0 C240,44.5 300.0,44.5 300.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-78709a30c196493fb87e650e325bf424-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M300.0,89.0 L308.0,77.0 292.0,77.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-78709a30c196493fb87e650e325bf424-0-3\" stroke-width=\"2px\" d=\"M325,87.0 C325,44.5 385.0,44.5 385.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-78709a30c196493fb87e650e325bf424-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M385.0,89.0 L393.0,77.0 377.0,77.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-78709a30c196493fb87e650e325bf424-0-4\" stroke-width=\"2px\" d=\"M495,87.0 C495,44.5 555.0,44.5 555.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-78709a30c196493fb87e650e325bf424-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M495,89.0 L487,77.0 503,77.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-78709a30c196493fb87e650e325bf424-0-5\" stroke-width=\"2px\" d=\"M410,87.0 C410,2.0 560.0,2.0 560.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-78709a30c196493fb87e650e325bf424-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M560.0,89.0 L568.0,77.0 552.0,77.0\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"]},"metadata":{}}],"execution_count":7,"metadata":{"id":"IMLaowjn5M9q","outputId":"8d4f5c88-d3df-4c05-801f-a0b7640ce901","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"ok","timestamp":1698047767495,"user_tz":-120,"elapsed":24,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":["# create another sentence\n","sentence2 = sp(u\"She isn't looking to buy an apartment.\")\n","\n","# and we print out the dependences\n","for word in sentence2:\n","    print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')"],"outputs":[{"output_type":"stream","name":"stdout","text":["She          PRON       PRP      pronoun, personal\n","is           AUX        VBZ      verb, 3rd person singular present\n","n't          PART       RB       adverb\n","looking      VERB       VBG      verb, gerund or present participle\n","to           PART       TO       infinitival \"to\"\n","buy          VERB       VB       verb, base form\n","an           DET        DT       determiner\n","apartment    NOUN       NN       noun, singular or mass\n",".            PUNCT      .        punctuation mark, sentence closer\n"]}],"execution_count":9,"metadata":{"id":"HPBKmlWoj_UL","outputId":"a1c31e93-565d-4839-dfbe-222257b9afec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047801404,"user_tz":-120,"elapsed":6,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["Notice that the isn't becomes 2 tokens.\n","\n","You can also break down a document in sentences."],"metadata":{"id":"1qGn6ITclB_T"}},{"cell_type":"code","source":["document = sp(u'The Big Bang model describes the universe from its very first moments.  Even today, this ancient radiation is all around us.')\n","for i,sentence in enumerate(document.sents):\n","    print(i, \":\", sentence);"],"outputs":[{"output_type":"stream","name":"stdout","text":["0 : The Big Bang model describes the universe from its very first moments.  \n","1 : Even today, this ancient radiation is all around us.\n"]}],"execution_count":10,"metadata":{"id":"MiNYQyCwkxkp","outputId":"50afd1c9-a84c-4961-b526-6344cf6e9a8d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047804068,"user_tz":-120,"elapsed":9,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":["# more elaborate tokenization\n","sentence3 = sp(u'I\\'m leaving U.K. for U.S.A.')\n","for word in sentence3:\n","    print(word.text)"],"outputs":[{"output_type":"stream","name":"stdout","text":["I\n","'m\n","leaving\n","U.K.\n","for\n","U.S.A.\n"]}],"execution_count":11,"metadata":{"id":"wOC6aqzhmHP2","outputId":"ce5434a2-e991-4f96-b76a-dc32a9579d1d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047805847,"user_tz":-120,"elapsed":243,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["You see that U.K. and U.S.A. are correctly recognized as different token and not split into several ones."],"metadata":{"id":"UItT9wCBmVIP"}},{"cell_type":"code","source":["# and another one\n","\n","sentence4 = sp(u\"Hello, I am Michalis from Zurich, Switzerland, email me at michalis@gmail.com\")\n","for word in sentence4:\n","    print(word.text)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Hello\n",",\n","I\n","am\n","Michalis\n","from\n","Zurich\n",",\n","Switzerland\n",",\n","email\n","me\n","at\n","michalis@gmail.com\n"]}],"execution_count":12,"metadata":{"id":"axX9QozemfsR","outputId":"966a6e36-894e-42cd-d44a-44750152a567","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047808024,"user_tz":-120,"elapsed":259,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["The email was correctly recognized as one token."],"metadata":{"id":"I5_r7mPSmuPb"}},{"cell_type":"markdown","source":["## Stemming\n","\n","Stemming refers to reducing a word to its root form. While performing natural language processing tasks, you will encounter various scenarios where you find different words with the same root. For instance, compute, computer, computing, computed, etc. You may want to reduce the words to their root form for the sake of uniformity. This is where stemming comes in to play.\n","\n","SpaCy does not include stemming; we will use NLTK. The most popular stemmer (for English) is the \"Porter Stemmer\"."],"metadata":{"id":"MZMOrvKqoQEt"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem.porter import *\n","\n","stemmer = PorterStemmer()\n","tokens = ['compute', 'computer', 'computed', 'computing']\n","for token in tokens:\n","    print(token + ' --> ' + stemmer.stem(token))"],"outputs":[{"output_type":"stream","name":"stdout","text":["compute --> comput\n","computer --> comput\n","computed --> comput\n","computing --> comput\n"]}],"execution_count":13,"metadata":{"id":"-TJuS5lkng47","outputId":"020447bd-cc4a-45c0-e922-c80a6d44d674","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047811517,"user_tz":-120,"elapsed":1723,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Lemmatization\n","\n","Lemmatization is less aggressive than stemming."],"metadata":{"id":"2lqFI9MJE6A1"}},{"cell_type":"code","source":["sentence = sp(u'run runs running runner talks talk talking talked')\n","\n","for word in sentence:\n","    print(word.text,\" --> \", word.lemma_)"],"outputs":[{"output_type":"stream","name":"stdout","text":["run  -->  run\n","runs  -->  run\n","running  -->  run\n","runner  -->  runner\n","talks  -->  talk\n","talk  -->  talk\n","talking  -->  talk\n","talked  -->  talk\n"]}],"execution_count":14,"metadata":{"id":"sTj0nCsDrk-o","outputId":"c210a8f3-8beb-4c9f-b93d-82743610ec6e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047811518,"user_tz":-120,"elapsed":6,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Stopwords\n","\n","Stop words are English words such as \"the\", \"a\", \"an\" etc that do not have any meaning of their own. Stop words are often not very useful for NLP tasks such as text classification or language modeling. So it is often better to remove these stop words before further processing of the document.\n","\n","The spaCy library contains 305 stop words. In addition, depending upon our requirements, we can also add or remove stop words from the spaCy library.\n","\n","To see the default spaCy stop words, we can use stop_words attribute of the spaCy model as shown below:"],"metadata":{"id":"h2dVidUXJYo0"}},{"cell_type":"code","source":["spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n","\n","#Printing the total number of stop words:\n","print('Number of stop words: %d' % len(spacy_stopwords))\n","\n","#Printing first ten stop words:\n","print('First ten stop words: %s' % list(spacy_stopwords)[:20])"],"outputs":[{"output_type":"stream","name":"stdout","text":["Number of stop words: 326\n","First ten stop words: ['behind', 'get', 'enough', 'if', 'us', 'his', 'was', 'therefore', 'above', 'so', 'down', 'nobody', 'the', 'only', 'each', 'off', 'still', 'due', 'twelve', \"n't\"]\n"]}],"execution_count":15,"metadata":{"id":"uU3wJ72s84oU","outputId":"153c357e-5a3e-4770-da8c-c6fa45fbc555","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047814788,"user_tz":-120,"elapsed":4,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":["# typically we should remove stopwords from our text.\n","\n","text = \"There are many documents that contain stopwords they are not very useful\"\n","filtered_sentence=[]\n","doc = sp(text)\n","\n","# filtering stop words\n","for word in doc:\n","    if word.is_stop==False:\n","        filtered_sentence.append(word)\n","print(\"Filtered Sentence:\",filtered_sentence)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered Sentence: [documents, contain, stopwords, useful]\n"]}],"execution_count":16,"metadata":{"id":"G6clFzRI9vZP","outputId":"420fdd5b-98b8-4678-dc70-a8ec6d4c3904","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047816281,"user_tz":-120,"elapsed":5,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":["# is a word a stopword?\n","sp.vocab['wonder'].is_stop"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":17}],"execution_count":17,"metadata":{"id":"kgC6FqFP-V7j","outputId":"8bc81db2-d3a5-4198-f94d-ce0991ba5b04","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047817527,"user_tz":-120,"elapsed":252,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Detecting entities\n","\n","While we are at it, we can see that it's very easy to detect entities (this is called **Named Entity Recognition**. SpaCy, comes with a pre-trained classifier that detects important entities: location, time, people, money etc.\n","\n","To get the named entities from a document, you have to use the `ents` attribute. Let's retrieve the named entities from the above sentence. Execute the following script:"],"metadata":{"id":"eiHLtf4fl8PU"}},{"cell_type":"code","source":["for entity in sentence4.ents:\n","    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Zurich - GPE - Countries, cities, states\n","Switzerland - GPE - Countries, cities, states\n"]}],"execution_count":18,"metadata":{"id":"1Z3CT7eAlnbY","outputId":"96b79654-8afa-47ac-b60c-b827f8318f07","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047820593,"user_tz":-120,"elapsed":233,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Exercise\n","\n","Use the sentence below.\n","\n","- How many tokens does it have?\n","- How many entities are recognized?\n"],"metadata":{"id":"XqRr67YpqK2o"}},{"cell_type":"code","source":["sentence = 'This year''s Nobel Prize in economics was awarded to three scholars who revolutionized the effort to end global poverty: Abhijit Banerjee and Esther Duflo of MIT and Michael Kremer of Harvard are essentially credited with applying the scientific method to an enterprise that, until recently, was largely based on gut instincts.'\n","\n","# show them. how many tokens\n","words = sentence.split()\n","word_count = len(words)\n","\n","# show them. how many entities\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(sentence)\n","entity_count = 0\n","for entity in doc.ents:\n","    print(entity)\n","    entity_count += 1\n","\n","######\n","\n","print(\"Number of words : \", word_count)\n","print(\"Number of entities : \", entity_count) #not really sure about it\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["This years\n","Nobel Prize\n","three\n","Abhijit Banerjee\n","Esther Duflo\n","MIT\n","Michael Kremer\n","Harvard\n","Number of words :  51\n","Number of entities :  8\n"]}],"execution_count":21,"metadata":{"id":"baDKZ2EVqPfp","outputId":"a3e499b4-709f-48ea-d693-da76a2c373c7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698047850842,"user_tz":-120,"elapsed":1722,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Advanced Entities and showing them in the text\n","\n","Of course there are many more entities that we can detect in a text. Let's see an example."],"metadata":{"id":"jFL5cELt6OAk"}},{"cell_type":"code","source":["from spacy import displacy\n","\n","nytimes= sp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n","\n","At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n","\n","The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n","\n","entities=[(i, i.label_, i.label) for i in nytimes.ents]\n","entities"],"outputs":[],"execution_count":null,"metadata":{"id":"qcEnXXOY6RRv","executionInfo":{"status":"aborted","timestamp":1698047767786,"user_tz":-120,"elapsed":4,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":["displacy.render(nytimes, style = \"ent\",jupyter = True)"],"outputs":[],"execution_count":null,"metadata":{"id":"kIIM1Ro16heq","executionInfo":{"status":"aborted","timestamp":1698047767787,"user_tz":-120,"elapsed":57052,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["You can see that all the 4 words have been reduced to \"comput\" which actually isn't a word (but it can be considered as a token), and it does show that all 4 words have something in common."],"metadata":{"id":"4i5OlqStruh-"}},{"cell_type":"markdown","source":["# Text Representation (Bag of words)"],"metadata":{"id":"JIPLS2BD-gHM"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","texts = [ \"I like the Matrix and the Patriot\", \"I did not like the Ants movie\", \"I hate comedies, but the \"]\n","# texts = [\n","#     \"Walks like a duck, talks like a duck\",\n","#     \"Beijing duck is the dish I like\",\n","#     \"Roger Rabbit has the recipe of success\",\n","#     \"A recipe for rabbit\",\n","#     \"A recipe for Beijing duck\"\n","# ]\n","\n","# using default tokenizer\n","count = CountVectorizer(ngram_range=(1,2))\n","bow = count.fit_transform(texts)\n","\n","# Show feature matrix\n","bow.toarray()"],"outputs":[],"execution_count":null,"metadata":{"id":"kXu3Cpcl-iyL","executionInfo":{"status":"aborted","timestamp":1698047767789,"user_tz":-120,"elapsed":57048,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":["# Get feature names\n","feature_names = count.get_feature_names()\n","\n","# View feature names\n","feature_names"],"outputs":[],"execution_count":null,"metadata":{"id":"Hdyz6l9Bmcum","executionInfo":{"status":"aborted","timestamp":1698047767789,"user_tz":-120,"elapsed":57042,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":["# show as a dataframe\n","pd.DataFrame(\n","    bow.todense(),\n","    columns=feature_names\n","    )"],"outputs":[],"execution_count":null,"metadata":{"id":"HKceAKN9mhn5","executionInfo":{"status":"aborted","timestamp":1698047767790,"user_tz":-120,"elapsed":57038,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Exercise:\n","\n","Above, we only used 1-grams (each word on each own). Change the code above to create a bag-of-words representation that includes\n","\n","    A. 1-grams and 2-grams.\n","    B. 1,2,3-grams\n","\n","**Hint:** Use the `ngram_range` parameter in the `CountVectorizer`.\n","\n","What do you notice? How many features does your document-term matrix have now?\n"],"metadata":{"id":"GMM4B-Njo6_l"}},{"cell_type":"markdown","source":["## TF-IDF Representation\n","\n","Recall that:\n","\n","- term frequency tf = count(word, document) / len(document)\n","- term frequency idf = log( len(collection) / count(document_containing_term, collection) )\n","- tf-idf = tf * idf\n","\n","It is important to mention that the IDF value for a word remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a word differ from document to document.\n","\n","The TF for the word \"car\" is 1/7.\n","\n","Let's find the IDF frequency of the word \"car\". Since we have 2 documents and the word \"car\" occurs in 1 of them, therefore the IDF value of the word \"car\" is log(2/1) = 1.66.\n","\n","\n","\n","Finally, the TF-IDF values are calculated by multiplying TF values with their corresponding IDF values.\n","\n","**Note**: In the example below, you may not get the exact values by multiplying those two numbers, because nltk normalizes each row to have norm of 1. However the relative importance of the terms won't change.\n","\n"],"metadata":{"id":"8P98scElj88b"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","texts = [\n","    \"The car is driven on the road.\",\n","    \"The truck is driven on the highway\"\n","]\n","# using default tokenizer in TfidfVectorizer\n","tfidf = TfidfVectorizer(ngram_range=(1, 1))\n","features = tfidf.fit_transform(texts)\n","pd.DataFrame(\n","    features.todense(),\n","    columns=tfidf.get_feature_names()\n",")"],"outputs":[],"execution_count":null,"metadata":{"id":"hP06zKDdj8aB","executionInfo":{"status":"aborted","timestamp":1698047767790,"user_tz":-120,"elapsed":57032,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"markdown","source":["## Combining NLTK and spaCy\n","\n","Of course you can combine the two tools."],"metadata":{"id":"JKBL30NxyA2l"}},{"cell_type":"code","source":["import spacy\n","import pandas as pd\n","from html import unescape\n","\n","# create a dataframe from a word matrix\n","def wordmatrix_2_df(wm, feat_names):\n","    # create an index for each row\n","    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n","    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n","                      columns=feat_names)\n","    return(df)\n","\n","# create a spaCy tokenizer\n","spacy.load('en')\n","lemmatizer = spacy.lang.en.English()\n","\n","# remove html entities from docs and\n","# set everything to lowercase\n","def my_preprocessor(doc):\n","    return(unescape(doc).lower())\n","\n","# tokenize the doc and lemmatize its tokens\n","def my_tokenizer(doc):\n","    tokens = lemmatizer(doc)\n","    return([token.lemma_ for token in tokens])\n","\n","corpora = ['University of Lausanne', 'University of Geneva', 'University of Zurich']\n","\n","custom_vec = CountVectorizer(preprocessor=my_preprocessor, tokenizer=my_tokenizer)\n","cwm = custom_vec.fit_transform(corpora)\n","tokens = custom_vec.get_feature_names()\n","wordmatrix_2_df(cwm, tokens)"],"outputs":[],"execution_count":null,"metadata":{"id":"1mgXgAcPkAwy","executionInfo":{"status":"aborted","timestamp":1698047767790,"user_tz":-120,"elapsed":57026,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"id":"mEmheRmJyueh","executionInfo":{"status":"aborted","timestamp":1698047767791,"user_tz":-120,"elapsed":57025,"user":{"displayName":"The Brisly (laurafabgomez)","userId":"15217668713777802930"}}}}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ahmadajal/DM_ML_course_public/blob/master/6.%20Text%20Analytics/in-classExercise/Text_Analytics_Tokenization_Stemming_Lemmatization.ipynb","timestamp":1698047454067}]},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"kernel_info":{"name":"python3"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"0.15.0"}},"nbformat":4,"nbformat_minor":0}