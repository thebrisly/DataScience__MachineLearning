# -*- coding: utf-8 -*-
"""Assignment_part_five_LauraFabbiano.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nHp6vntbtCixqZGwsd4FBh9yf0RUOE-L

<a href="https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%205/Assignment_part_five.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

DSML investigation:

You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.

Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!

# Assignement part five

### Due 29.10 (You get an extra hour!)

By now you should have 4 suspects left.
More information came in that suggests that the rogue agent is tampering with the sentiment annotation system of the SIMF which analyses news documents and marks their sentiment of intelligence analysis tasks.

This annotation is crutial to identify documents expressing negativity towards Switzerland and its allies.

Each document contains a column which shows which user accessed it. We know that the rogue agent accessed only the documents whose negative sentiment was high, and was then changed to positive or neutral. We will use a huggingface model to identify what records have been tampered with.


[You can find more models on this link](https://huggingface.co/models?sort=trending)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets transformers huggingface_hub
# !apt-get install git-lfs
# !pip install transformers[torch]
# !pip install accelerate -U
# # Import required packages
# 
# # library for huggingface
# from transformers import pipeline, DataCollatorWithPadding
# 
# # other libraries
# import pandas as pd
# import numpy as np
# import torch
# import spacy
# from sklearn.model_selection import train_test_split
# 
# torch.cuda.is_available()
#

"""# 1. Getting to know our data"""

df = pd.read_excel('https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%205/data/Reduced_Set_2100.xlsx')

df.head(5)

df.dtypes

df.shape

df["evaluation"].value_counts()

"""# 2. Re-evaluating with SIMF's model:
Evaluate the sentiment on the title column using a sentiment pipeline trained on the `finiteautomata/bertweet-base-sentiment-analysis`model


_This may take a while_
"""

# initialisation of the sentiment analysis pipeline based on the model bertweet :

sentiment_pipeline =  pipeline('sentiment-analysis', model='finiteautomata/bertweet-base-sentiment-analysis')

# testing the sentiment model to know it better :

test_pos = sentiment_pipeline("I love this course even if the assignment is sometimes not so clear")
test_neg = sentiment_pipeline("I hate this course")
test_mid = sentiment_pipeline("I don't know what to think about this course but i may be surprised")

print(test_pos)
print(test_neg)
print(test_mid)

# just to do it in a cleaner way
def longer_sentiment(label):
    if label == 'POS':
        return 'positive'
    elif label == 'NEG':
        return 'negative'
    elif label == 'NEU':
        return 'neutral'

# evaluating the sentiment on the title colum using the sentiment pipeline :
new_evaluation = []

for title in df['title']:
    result = sentiment_pipeline(title)
    cleaner_result = longer_sentiment(result[0]['label']) # using the function that i created above to clean my data :)
    new_evaluation.append(cleaner_result)

# adding the new_eval next to the current evaluations
df.insert(df.columns.get_loc("evaluation") + 1, "new_evaluation", new_evaluation)

# display the new database :

df.head()

# check how many values of each there are now (for new_evaluation)

df["new_evaluation"].value_counts()

"""## 2.1 How many of the total entries match both the SIMF model **and** the hugginface model?"""

# we will use the eq() function to answer this question :

is_equal = df['evaluation'].eq(df['new_evaluation'])
matching_table = df[is_equal]
matching_df = matching_table.copy()
matching_entries = is_equal.sum()

print("Number of entries that match : ", matching_entries)
print("In % : ", ((matching_entries / len(df)))) # 913 / 2100

"""## 2.2 We will now focus on the entries that do not match
#### Identify all non matching entries
"""

# identifying all the non matching entries
does_not_match = df['evaluation'] != df['new_evaluation']

# creating
non_matching_table = df[does_not_match]
non_matching_df = non_matching_table.copy()

# displaying the new df
print("Number of non matching entries :", len(non_matching_df))
non_matching_df.head()

"""## 2.3 How many of those entries that our model predicted as negative, are evaluate as neutral or positive by the SIMF model ?

Store the resulting dataframe into a new one that we will be using in the following questions.
"""

# selecting all the negative values predicted in our model (new_evaluation) that have a corresponding positive/neutral evaluation
selected = (non_matching_df['new_evaluation'] == 'negative') & ((non_matching_df['evaluation'] == 'positive') | (non_matching_df['evaluation'] == 'neutral'))

sel = non_matching_df[selected]
altered_df = sel.copy()

print("Number of positive/neutral values that have been predicted as negative in our model : ", len(altered_df))

# the new data frame is new_df and looks like that :
print(altered_df.shape)
display(altered_df)

"""# 3. Use the ChangeLog dataframe to identify the usersID's who edited the tampered entries, and only the altered entries"""

ChangeLog = pd.read_csv('https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%205/data/ChangeLog.csv')

# knowing the data
ChangeLog.head()

ChangeLog.shape

"""## 3.1 Identifying the users who have edited tampered documents"""

sus_edited_tampered = non_matching_df.merge(ChangeLog, on='title', how='inner')

# just checking how many users we found and how many unique users:
suspects_list = sus_edited_tampered['UserID']
unique_sl_count = suspects_list.nunique()

print("Number of userID found : ", len(suspects_list))
print("Number of unique users : ", unique_sl_count)

"""## 3.2 Identifying the users who have edited non-tampered documents"""

sus_edited_nontampered = matching_df.merge(ChangeLog, on='title', how='inner')

# just checking how many users we found and how many unique users:
non_suspects_list = sus_edited_nontampered['UserID']
unique_nsl_count = non_suspects_list.nunique()

print("Number of userID found: ", len(non_suspects_list))
print("Number of unique users : ", unique_nsl_count)

"""## 3.3 combining the results from `3.1` and `3.2` to identify users who only edited tampered documents.
These are our suspects.
"""

suspects_set = set(suspects_list)
non_suspects_set = set(non_suspects_list)

cleaned_suspects_set = suspects_set - non_suspects_set
len(cleaned_suspects_set)

# answering to the question on moodle :

real_suspects_list = [int(suspect[1:-1]) for suspect in cleaned_suspects_set] # asked chatgpt to do that otherwise i had a problem

for suspect in [241540, 754702, 527013, 223968, 152304] :
  if suspect in real_suspects_list :
    print(suspect)

"""# 4. Identifying important informations on the altered documents.

In this section we will use the TF-IDF text representation model to identify other important information on the altered documents.

----
[note to myself]


Altered document = the data that don't match the value in both model and that have been converted from negative to positive/neutral
"""

# Make a list of the text within articles with the original dataset (the one of section 1)
articles_list = df['news'].tolist()

# checking if the list of articles has been correctly implemented
display(articles_list[0])

print("\n The number of articles is : ", len(articles_list))

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Using default tokenizer in TfidfVectorizer, use the "english" stop words, and unigrams
default_tokenizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))

# Learn the vocabulary dictionary and return document-term matrix
tfidf_matrix = default_tokenizer.fit_transform(articles_list)

# Visualize result in dataframe
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=default_tokenizer.get_feature_names_out())

display(tfidf_df)

# just to make sure that there are not only zero-values as the table is really big :
non_zero_count = tfidf_matrix.nnz
print("Number of non-zero values in the TF-IDF matrix :", non_zero_count)

# Keep the entries related to tampered documents (in my case : altered_df)
altered_articles_list = altered_df['news'].tolist()

tfidf_altered_matrix = default_tokenizer.transform(altered_articles_list) # here i will only tranform, as i don't want to train the model again.

# getting the features names
feature_names = default_tokenizer.get_feature_names_out()

# Identify the record that stands out the most on the altered documents (you can use the sum of the tokenizers results)
sum_word_scores = tfidf_altered_matrix.sum(axis=0)
sum_word_scores_df = pd.DataFrame(sum_word_scores, columns=feature_names)

# displaying the 4 most important words :
top_words = sum_word_scores_df.iloc[0].sort_values(ascending=False).head(4)

print("Here are the top 4 words of all the altered texts\n", top_words)

# How many records contain the word that stands out the most?
# e.g. if the word that stood out the most was "mouton", how many of the altered records contain the word mouton.
# How about the second word that stands out the most
# How about the third ?
# How about the fourth ?

top_words = top_words.index

# dictionnary init to count the number of occurences
word_counts = {word: 0 for word in top_words}

# for each word, checking and counting how many times it appears in the articles :
for article in altered_articles_list:
    for word in top_words:
        if word in article:
            word_counts[word] += 1

# displaying the number of times that a word appears
for word, count in word_counts.items():
    print("the word ", word, " is found in ", count ," altered articles")

"""#### Moodle quizz: if the order of frequency in appearance, did not match the values assigned by the tokenizer, is it normal?

My answer : yes it is because some words can be more rare than others so it will have a better tf-idf scores than some words that appear more times and that are more popular. For example, the word carbon is really specific, so it's normal that it has a better score than said that is a really common word and can be found everywhere.

<h1> Which users have been suspects for all parts so far? </h1>
Choose those that apply,

You may find on moodle a table summing up the potential suspects thus far.
"""

print(real_suspects_list)

for suspect in [410319, 785994, 638911, 628854, 793674] :
  if suspect in real_suspects_list :
    print(suspect)

# then compare them with the excel file and check if they appear in all parts :)